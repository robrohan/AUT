{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "feea0805",
   "metadata": {},
   "source": [
    "[Chunking](https://community.databricks.com/t5/technical-blog/the-ultimate-guide-to-chunking-strategies-for-rag-applications/ba-p/113089)\n",
    "- Fixed-Size Chunking (word, char or token counts (with overlaps))\n",
    "- Semantic Chunking (break at paragraphs or sentences)\n",
    "- Recursive Chunking\n",
    "- Adaptive Chunking\n",
    "- Context-Enriched Chunking\n",
    "- AI-Driven Dynamic Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "67c784ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU langchain-text-splitters transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ee17139e",
   "metadata": {},
   "outputs": [],
   "source": [
    "document = None\n",
    "with open(\"./datasets/dsm.md\", 'r', encoding='utf-8') as f:\n",
    "    document = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84cce59",
   "metadata": {},
   "source": [
    "## Fixed-Size Chunking\n",
    "\n",
    "This is the simplest method. This splits based on a given character sequence, which defaults to \"\\n\\n\". Chunk length is measured by number of characters.\n",
    "\n",
    "1. How the text is split: by single character separator.\n",
    "2. How the chunk size is measured: by number of characters.\n",
    "\n",
    "To obtain the string content directly, use .split_text.\n",
    "To create LangChain Document objects (e.g., for use in downstream tasks), use .create_documents.\n",
    "\n",
    "https://python.langchain.com/docs/how_to/character_text_splitter/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f334fb3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (653 > 512). Running this sequence through the model will result in indexing errors\n",
      "Created a chunk of size 653, which is longer than the specified 400\n",
      "Created a chunk of size 803, which is longer than the specified 400\n",
      "Created a chunk of size 416, which is longer than the specified 400\n",
      "Created a chunk of size 408, which is longer than the specified 400\n",
      "Created a chunk of size 478, which is longer than the specified 400\n",
      "Created a chunk of size 453, which is longer than the specified 400\n",
      "Created a chunk of size 472, which is longer than the specified 400\n",
      "Created a chunk of size 568, which is longer than the specified 400\n",
      "Created a chunk of size 635, which is longer than the specified 400\n",
      "Created a chunk of size 403, which is longer than the specified 400\n",
      "Created a chunk of size 453, which is longer than the specified 400\n",
      "Created a chunk of size 458, which is longer than the specified 400\n",
      "Created a chunk of size 1006, which is longer than the specified 400\n",
      "Created a chunk of size 584, which is longer than the specified 400\n",
      "Created a chunk of size 727, which is longer than the specified 400\n",
      "Created a chunk of size 803, which is longer than the specified 400\n",
      "Created a chunk of size 1136, which is longer than the specified 400\n",
      "Created a chunk of size 427, which is longer than the specified 400\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "# Load a tokenizer for a BERT-like model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "def count_tokens(text):\n",
    "    return len(tokenizer.encode(text))\n",
    "\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\",  # <=== new\n",
    "    chunk_size=400,\n",
    "    chunk_overlap=50,\n",
    "    # length_function=len,\n",
    "    length_function=count_tokens,\n",
    "    # is_separator_regex=False,\n",
    ")\n",
    "texts = text_splitter.create_documents([document])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e1aaac63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 416, which is longer than the specified 400\n",
      "Created a chunk of size 401, which is longer than the specified 400\n",
      "Created a chunk of size 473, which is longer than the specified 400\n",
      "Created a chunk of size 453, which is longer than the specified 400\n",
      "Created a chunk of size 458, which is longer than the specified 400\n",
      "Created a chunk of size 408, which is longer than the specified 400\n",
      "Created a chunk of size 478, which is longer than the specified 400\n",
      "Created a chunk of size 437, which is longer than the specified 400\n",
      "Created a chunk of size 403, which is longer than the specified 400\n",
      "Created a chunk of size 453, which is longer than the specified 400\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "directory = \"./sections\"\n",
    "for file in os.listdir(directory):\n",
    "    filename = os.fsdecode(file)\n",
    "    if filename.endswith(\".json\"):\n",
    "        with open(os.path.join(directory, filename)) as f:\n",
    "            jsn = json.loads(f.read())\n",
    "            section = jsn[\"section\"]\n",
    "            texts = text_splitter.create_documents([section])\n",
    "            chunks = []\n",
    "            for i,t in enumerate(texts):\n",
    "                # replace newlines with spaces this can help keep word boundires\n",
    "                chunks.append(t.page_content.replace(\"\\n\", \" \"))\n",
    "            jsn[\"chunks\"] = chunks\n",
    "\n",
    "            with open(f'./chunks/{jsn[\"id\"]}.json', \"w\") as wf:\n",
    "                wf.write(json.dumps(jsn))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa728fd",
   "metadata": {},
   "source": [
    "## Semantic Chunking\n",
    "\n",
    "This is a better chunking method, but for simplicity, I am just using the CharacterTextSplitter method above.\n",
    "\n",
    "This text splitter is the recommended one for generic text. It is parameterized by a list of characters. It tries to split on them in order until the chunks are small enough. The default list is [\"\\n\\n\", \"\\n\", \" \", \"\"]. This has the effect of trying to keep all paragraphs (and then sentences, and then words) together as long as possible, as those would generically seem to be the strongest semantically related pieces of text.\n",
    "\n",
    "1. How the text is split: by list of characters.\n",
    "2. How the chunk size is measured: by number of characters.\n",
    "\n",
    "Below we show example usage.\n",
    "\n",
    "To obtain the string content directly, use .split_text.\n",
    "\n",
    "To create LangChain Document objects (e.g., for use in downstream tasks), use .create_documents.\n",
    "\n",
    "https://python.langchain.com/docs/how_to/recursive_text_splitter/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f01c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# text_splitter = RecursiveCharacterTextSplitter(\n",
    "#     # separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],\n",
    "#     chunk_size=1000,\n",
    "#     chunk_overlap=20,\n",
    "#     length_function=len,\n",
    "#     is_separator_regex=False,\n",
    "# )\n",
    "\n",
    "# texts = text_splitter.create_documents([document])\n",
    "# print(len(texts))\n",
    "# print(texts[0])\n",
    "# print(texts[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spikes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
